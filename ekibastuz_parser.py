import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

ads_data = []
headers = {"User-Agent": "Mozilla/5.0"}

# ‚úÖ –°–æ–±–µ—Ä—ë–º 149 —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ –≠–∫–∏–±–∞—Å—Ç—É–∑—É
for page in range(1, 150):
    print(f'üîÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –∏–∑ 149')
    url = f'https://krisha.kz/prodazha/kvartiry/ekibastuz/?page={page}'

    try:
        response = requests.get(url, headers=headers)
        if response.status_code != 200:
            print(f'‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ {page} (–∫–æ–¥ {response.status_code})')
            continue

        soup = BeautifulSoup(response.text, 'html.parser')
        ads = soup.find_all('div', class_='a-card__inc')

        for ad in ads:
            try:
                title = ad.find('a', class_='a-card__title').text.strip()
                price = ad.find('div', class_='a-card__price').text.strip()
                location = ad.find('div', class_='a-card__subtitle').text.strip()
                link = "https://krisha.kz" + ad.find('a', class_='a-card__title')['href']

                # –†–∞–π–æ–Ω, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
                district = location.split(',')[-1].strip() if ',' in location else None

                ads_data.append({
                    'title': title,
                    'price': price,
                    'location': location,
                    'district': district,
                    'link': link
                })

            except Exception:
                continue

        time.sleep(1.2)

    except Exception as e:
        print(f'üö® –û—à–∏–±–∫–∞ –Ω–∞ {url}: {e}')
        continue

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—É —Å—Å—ã–ª–æ–∫
df = pd.DataFrame(ads_data)
df.to_csv('krisha_ekibastuz_full.csv', index=False, encoding='utf-8-sig')
print(f'‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(df)}')
